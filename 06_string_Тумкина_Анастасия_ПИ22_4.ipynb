{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHBY8hlE_ug"
      },
      "source": [
        "# Работа со строковыми значениями"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfr5WCOuE_uj"
      },
      "source": [
        "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
        "\n",
        "Материалы:\n",
        "* Макрушин С.В. Лекция \"Работа со строковыми значениям\"\n",
        "* https://pyformat.info/\n",
        "* https://docs.python.org/3/library/re.html\n",
        "    * https://docs.python.org/3/library/re.html#flags\n",
        "    * https://docs.python.org/3/library/re.html#functions\n",
        "* https://pythonru.com/primery/primery-primeneniya-regulyarnyh-vyrazheniy-v-python\n",
        "* https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/\n",
        "* https://realpython.com/nltk-nlp-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIUDwV8tE_uk"
      },
      "source": [
        "## Задачи для совместного разбора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnpHuWf7E_uk"
      },
      "source": [
        "1. Вывести на экран данные из словаря `obj` построчно в виде `k = v`, задав формат таким образом, чтобы знак равенства оказался на одной и той же позиции во всех строках. Строковые литералы обернуть в кавычки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQKhnWTXE_ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa5ea19-ae79-4e77-e2b5-42f154faca11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "home_page   = \"https://github.com/pypa/sampleproject\"\n",
            "keywords    = \"sample setuptools development\"\n",
            "license     = \"MIT\"\n"
          ]
        }
      ],
      "source": [
        "obj = {\n",
        "    \"home_page\": \"https://github.com/pypa/sampleproject\",\n",
        "    \"keywords\": \"sample setuptools development\",\n",
        "    \"license\": \"MIT\",\n",
        "}\n",
        "for k,v in obj.items():\n",
        "  print(f'{k: <11} = \"{v}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmXxM1gjE_ul"
      },
      "source": [
        "2. Написать регулярное выражение,которое позволит найти номера групп студентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdBLWLmeE_um",
        "outputId": "d24f07c6-3247-4bf2-dd63-a1bdce0ab2ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['гр.ПМ19-1', ' пм 20-4', ' 20-3']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "obj = pd.Series([\"Евгения гр.ПМ19-1\", \"Илья пм 20-4\", \"Анна 20-3\"])\n",
        "obj1 = ' '.join(obj)\n",
        "re.findall(r'гр\\.[а-яА-Я]{0,2} *[0-9]{2}-[1-9]| [а-яА-Я]{0,2} *[0-9]{2}-[1-9]', obj1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teP3bHRcE_um"
      },
      "source": [
        "3. Разбейте текст формулировки задачи 2 на слова."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in nltk.tokenize.word_tokenize('Написать регулярное выражение,которое позволит найти номера групп студентов.'):\n",
        "  if i.isalpha():\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F77XM6-3SEkH",
        "outputId": "f9ec0d97-7143-49f0-aab1-f52fd9622c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Написать\n",
            "регулярное\n",
            "выражение\n",
            "которое\n",
            "позволит\n",
            "найти\n",
            "номера\n",
            "групп\n",
            "студентов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g412mAvGE_un"
      },
      "source": [
        "## Лабораторная работа 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeMyWBxEE_un"
      },
      "source": [
        "### Форматирование строк"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iy4lXxM9WTL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8-G4XL5E_un"
      },
      "source": [
        "1\\. Загрузите данные из файла `recipes_sample.csv` (__ЛР2__) в виде `pd.DataFrame` `recipes` При помощи форматирования строк выведите информацию об id рецепта и времени выполнения 5 случайных рецептов в виде таблицы следующего вида:\n",
        "\n",
        "    \n",
        "    |      id      |  minutes  |\n",
        "    |--------------------------|\n",
        "    |    61178     |    65     |\n",
        "    |    202352    |    80     |\n",
        "    |    364322    |    150    |\n",
        "    |    26177     |    20     |\n",
        "    |    224785    |    35     |\n",
        "    \n",
        "Обратите внимание, что ширина столбцов заранее неизвестна и должна рассчитываться динамически, в зависимости от тех данных, которые были выбраны. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "recipes = pd.read_csv('recipes_sample.csv')\n",
        "id1 = pd.Series(random.choices(recipes['id'], k=5))\n",
        "print(f'|      id      |  minutes  |') \n",
        "print('|'+'-'*26+'|')\n",
        "for i in random.choices([i for i in range(0,len(recipes))], k=5):\n",
        "  print(f'|{recipes.id[i]: ^14}|{recipes.minutes[i]: ^11}|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-EJy23fVzLr",
        "outputId": "ddcc68eb-1193-4b44-df88-ecb30b94d1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|      id      |  minutes  |\n",
            "|--------------------------|\n",
            "|    327769    |    30     |\n",
            "|    95349     |    30     |\n",
            "|    307558    |    60     |\n",
            "|    239149    |    69     |\n",
            "|    227090    |    45     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recipes.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Aphqx6lW4cY3",
        "outputId": "41a45393-c923-4586-ef2f-2e2394b5db5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       name     id  minutes  contributor_id  \\\n",
              "0     george s at the cove  black bean soup  44123       90           35193   \n",
              "1        healthy for them  yogurt popsicles  67664       10           91970   \n",
              "2              i can t believe it s spinach  38798       30            1533   \n",
              "3                      italian  gut busters  35173       45           22724   \n",
              "4  love is in the air  beef fondue   sauces  84797       25            4470   \n",
              "\n",
              "    submitted  n_steps                                        description  \\\n",
              "0  2002-10-25      NaN  an original recipe created by chef scott meska...   \n",
              "1  2003-07-26      NaN  my children and their friends ask for my homem...   \n",
              "2  2002-08-29      NaN            these were so go, it surprised even me.   \n",
              "3  2002-07-27      NaN  my sister-in-law made these for us at a family...   \n",
              "4  2004-02-23      4.0  i think a fondue is a very romantic casual din...   \n",
              "\n",
              "   n_ingredients  \n",
              "0           18.0  \n",
              "1            NaN  \n",
              "2            8.0  \n",
              "3            NaN  \n",
              "4            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8df533a6-9a75-412c-8ee2-b1f473c78323\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>id</th>\n",
              "      <th>minutes</th>\n",
              "      <th>contributor_id</th>\n",
              "      <th>submitted</th>\n",
              "      <th>n_steps</th>\n",
              "      <th>description</th>\n",
              "      <th>n_ingredients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>george s at the cove  black bean soup</td>\n",
              "      <td>44123</td>\n",
              "      <td>90</td>\n",
              "      <td>35193</td>\n",
              "      <td>2002-10-25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>an original recipe created by chef scott meska...</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>healthy for them  yogurt popsicles</td>\n",
              "      <td>67664</td>\n",
              "      <td>10</td>\n",
              "      <td>91970</td>\n",
              "      <td>2003-07-26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my children and their friends ask for my homem...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i can t believe it s spinach</td>\n",
              "      <td>38798</td>\n",
              "      <td>30</td>\n",
              "      <td>1533</td>\n",
              "      <td>2002-08-29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>these were so go, it surprised even me.</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>italian  gut busters</td>\n",
              "      <td>35173</td>\n",
              "      <td>45</td>\n",
              "      <td>22724</td>\n",
              "      <td>2002-07-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my sister-in-law made these for us at a family...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>love is in the air  beef fondue   sauces</td>\n",
              "      <td>84797</td>\n",
              "      <td>25</td>\n",
              "      <td>4470</td>\n",
              "      <td>2004-02-23</td>\n",
              "      <td>4.0</td>\n",
              "      <td>i think a fondue is a very romantic casual din...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8df533a6-9a75-412c-8ee2-b1f473c78323')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8df533a6-9a75-412c-8ee2-b1f473c78323 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8df533a6-9a75-412c-8ee2-b1f473c78323');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K00JG3cE_uo"
      },
      "source": [
        "2\\. Напишите функцию `show_info`, которая по данным о рецепте создает строку (в смысле объекта python) с описанием следующего вида:\n",
        "\n",
        "```\n",
        "\"Название Из Нескольких Слов\"\n",
        "\n",
        "1. Шаг 1\n",
        "2. Шаг 2\n",
        "----------\n",
        "Автор: contributor_id\n",
        "Среднее время приготовления: minutes минут\n",
        "```\n",
        "\n",
        "    \n",
        "Данные для создания строки получите из файлов `recipes_sample.csv` (__ЛР2__) и `steps_sample.xml` (__ЛР3__). \n",
        "Вызовите данную функцию для рецепта с id `170895` и выведите (через `print`) полученную строку на экран."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "0izCAfPTktCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/steps_sample.xml') as f:\n",
        "    ab = BeautifulSoup(f, 'xml')"
      ],
      "metadata": {
        "id": "ba9aSlaMls77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps1 = []\n",
        "for i in ab.recipes.find_all(\"recipe\"):\n",
        "  if i.find(\"id\").next == '170895':\n",
        "    steps1 = [steps.next for steps in i.steps.find_all(\"step\")]\n",
        "\n",
        "for i in recipes.index:\n",
        "  if recipes.id[i] == 170895:\n",
        "    ind = i\n",
        "\n",
        "def show_info(name, steps, author, time):\n",
        "  steps = [f'{i+1}. {steps[i]}' for i in range(len(steps))]\n",
        "  return f'\"{name}\"\\n\\n' + f'\\n'.join(steps) + '\\n' + '-'*30 + \\\n",
        "  f'\\nАвтор: {author}\\nСреднее время приготовления: {time} минут'\n",
        "\n",
        "\n",
        "print(show_info(recipes.name[ind], steps1, recipes.contributor_id[ind], recipes.minutes[ind]))"
      ],
      "metadata": {
        "id": "0d1BnQZhgWbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1593ce9a-2436-4ebe-9565-a3596a899b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"leeks and parsnips  sauteed or creamed\"\n",
            "\n",
            "1. clean the leeks and discard the dark green portions\n",
            "2. cut the leeks lengthwise then into one-inch pieces\n",
            "3. melt the butter in a medium skillet , med\n",
            "4. heat\n",
            "5. add the garlic and fry 'til fragrant\n",
            "6. add leeks and fry until the leeks are tender , about 6-minutes\n",
            "7. meanwhile , peel and chunk the parsnips into one-inch pieces\n",
            "8. place in a steaming basket and steam 'til they are as tender as you prefer\n",
            "9. i like them fork-tender\n",
            "10. drain parsnips and add to the skillet with the leeks\n",
            "11. add salt and pepper\n",
            "12. gently sautee together for 5-minutes\n",
            "13. at this point you can serve it , or continue on and cream it:\n",
            "14. in a jar with a screw top , add the half-n-half and arrowroot\n",
            "15. shake 'til blended\n",
            "16. turn heat to low under the leeks and parsnips\n",
            "17. pour in the arrowroot mixture , stirring gently as you pour\n",
            "18. if too thick , gradually add the water\n",
            "19. let simmer for a couple of minutes\n",
            "20. taste to adjust seasoning , probably an additional 1 / 2 teaspoon salt\n",
            "21. serve warm\n",
            "------------------------------\n",
            "Автор: 8377\n",
            "Среднее время приготовления: 27 минут\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmiL88T_E_uo"
      },
      "outputs": [],
      "source": [
        "assert (\n",
        "    show_info(\n",
        "        name=\"george s at the cove black bean soup\",\n",
        "        steps=[\n",
        "            \"clean the leeks and discard the dark green portions\",\n",
        "            \"cut the leeks lengthwise then into one-inch pieces\",\n",
        "            \"melt the butter in a medium skillet , med\",\n",
        "        ],\n",
        "        minutes=90,\n",
        "        author_id=35193,\n",
        "    )\n",
        "    == '\"George S At The Cove Black Bean Soup\"\\n\\n1. Clean the leeks and discard the dark green portions\\n2. Cut the leeks lengthwise then into one-inch pieces\\n3. Melt the butter in a medium skillet , med\\n----------\\nАвтор: 35193\\nСреднее время приготовления: 90 минут\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHhyTZcYE_uo"
      },
      "source": [
        "## Работа с регулярными выражениями"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9KVfQVDE_up"
      },
      "source": [
        "3\\. Напишите регулярное выражение, которое ищет следующий паттерн в строке: число (1 цифра или более), затем пробел, затем слова: hour или hours или minute или minutes. Произведите поиск по данному регулярному выражению в каждом шаге рецепта с id 25082. Выведите на экран все непустые результаты, найденные по данному шаблону."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "fn4ukY-2Dnjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ab.recipes.find_all(\"recipe\"):\n",
        "  if i.find(\"id\").next == '25082':\n",
        "    steps1 = [steps.next for steps in i.steps.find_all(\"step\")]\n",
        "step = ''.join(steps1)\n",
        "re.findall(r'\\d+ hours|\\d+ minutes|\\d+ hour|\\d+ minute', step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LGEllfnq-by",
        "outputId": "e477a37e-9e22-49bf-9e87-05b6d4d1723a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['20 minutes',\n",
              " '10 minutes',\n",
              " '2 hours',\n",
              " '10 minutes',\n",
              " '20 minutes',\n",
              " '30 minutes']"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzWW4ol7E_up"
      },
      "source": [
        "4\\. Напишите регулярное выражение, которое ищет шаблон вида \"this..., but\" _в начале строки_ . Между словом \"this\" и частью \", but\" может находиться произвольное число букв, цифр, знаков подчеркивания и пробелов. Никаких других символов вместо многоточия быть не может. Пробел между запятой и словом \"but\" может присутствовать или отсутствовать.\n",
        "\n",
        "Используя строковые методы `pd.Series`, выясните, для каких рецептов данный шаблон содержится в тексте описания. Выведите на экран количество таких рецептов и 3 примера подходящих описаний (текст описания должен быть виден на экране полностью)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bool(re.findall(r'this *\\w*, *but', 'this    ,but'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9CBOWllvcza",
        "outputId": "ad1c9a94-bfe7-4805-815e-70aee1804de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 0\n",
        "for i in recipes.description:\n",
        "  if bool(re.match(r'this *\\w*, *but', str(i))):\n",
        "    k+=1\n",
        "    if k<=3:\n",
        "      print(f'{k}) {i}')\n",
        "print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfwQqlQI2J2",
        "outputId": "1d2ac844-39d7-4b24-9e14-6d10876495ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aegO9I7lE_up"
      },
      "source": [
        "5\\. В текстах шагов рецептов обыкновенные дроби имеют вид \"a / b\". Используя регулярные выражения, уберите в тексте шагов рецепта с id 72367 пробелы до и после символа дроби. Выведите на экран шаги этого рецепта после их изменения."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ab.recipes.find_all(\"recipe\"):\n",
        "  if i.find(\"id\").next == '72367':\n",
        "    steps2 = [steps.next for steps in i.steps.find_all(\"step\")]\n",
        "for i in steps2:\n",
        "  if bool(re.findall(r'\\d+ \\/ \\d+', i)):\n",
        "    i = re.sub(r' / ', '/', i)\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0atY7aspsbu9",
        "outputId": "90b2ea8a-fd5a-4283-fee8-9cc39d0c91f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mix butter , flour , 1/3 c\n",
            "sugar and 1-1/4 t\n",
            "vanilla\n",
            "press into greased 9\" springform pan\n",
            "mix cream cheese , 1/4 c\n",
            "sugar , eggs and 1/2 t\n",
            "vanilla beating until fluffy\n",
            "pour over dough\n",
            "combine apples , 1/3 c\n",
            "sugar and cinnamon\n",
            "arrange on top of cream cheese mixture and sprinkle with almonds\n",
            "bake at 350 for 45-55 minutes , or until tester comes out clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOQwz5EZE_up"
      },
      "source": [
        "### Сегментация текста"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "kEsBosyEeBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV2VAlr5eNOq",
        "outputId": "0b9a844a-d5c9-478a-f18d-2360a2611e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upE_Or8NE_up"
      },
      "source": [
        "6\\. Разбейте тексты шагов рецептов на слова при помощи пакета `nltk`. Посчитайте и выведите на экран кол-во уникальных слов среди всех рецептов. Словом называется любая последовательность алфавитных символов (для проверки можно воспользоваться `str.isalpha`). При подсчете количества уникальных слов не учитывайте регистр."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_words = set()\n",
        "for i in ab.recipes.find_all(\"recipe\"):\n",
        "  for steps in i.steps.find_all(\"step\"):\n",
        "    for j in nltk.tokenize.word_tokenize(steps.next):\n",
        "      if j.isalpha:\n",
        "        steps_words.add(j)\n",
        "len(steps_words)"
      ],
      "metadata": {
        "id": "2Czut8OqfCQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860d1ae9-246c-499a-99c1-ff504d65f70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19564"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps7SMWCoE_up"
      },
      "source": [
        "7\\. Разбейте описания рецептов из `recipes` на предложения при помощи пакета `nltk`. Найдите 5 самых длинных описаний (по количеству _предложений_) рецептов в датасете и выведите строки фрейма, соответствующие этим рецептами, в порядке убывания длины."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "ln = dict()\n",
        "for i in recipes.index:\n",
        "  if len(nltk.tokenize.sent_tokenize(str(recipes.description[i]))) > max_len:\n",
        "    if len(nltk.tokenize.sent_tokenize(str(recipes.description[i]))) not in ln.keys():\n",
        "      ln[len(nltk.tokenize.sent_tokenize(str(recipes.description[i])))] = i\n",
        "#len(nltk.tokenize.sent_tokenize(recipes.description[1345]))\n",
        "ln"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML1DwAUoGdmt",
        "outputId": "04d42456-9083-48bb-8fe7-ae61268aa9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{4: 0,\n",
              " 3: 1,\n",
              " 1: 2,\n",
              " 6: 4,\n",
              " 5: 6,\n",
              " 2: 11,\n",
              " 7: 15,\n",
              " 8: 43,\n",
              " 9: 111,\n",
              " 12: 127,\n",
              " 10: 185,\n",
              " 18: 401,\n",
              " 27: 481,\n",
              " 11: 670,\n",
              " 15: 860,\n",
              " 14: 1142,\n",
              " 13: 1393,\n",
              " 17: 2566,\n",
              " 20: 2760,\n",
              " 16: 4628,\n",
              " 23: 6779,\n",
              " 22: 14136,\n",
              " 76: 18408,\n",
              " 24: 22566}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln_sort = sorted(ln.items(), key=lambda x: x[0])\n",
        "for i in dict(ln_sort[-5:]).values():\n",
        "  print(recipes.iloc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVSCsm-pIqyp",
        "outputId": "ef2934d5-e289-40a7-9e82-62fca0905ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                 hot and sour soup  betty foo  hunan restaurant\n",
            "id                                                           141637\n",
            "minutes                                                          35\n",
            "contributor_id                                               167236\n",
            "submitted                                                2005-10-17\n",
            "n_steps                                                         NaN\n",
            "description       this is the recipe as taught in the main line ...\n",
            "n_ingredients                                                  19.0\n",
            "Name: 14136, dtype: object\n",
            "name                                                  chocolate tea\n",
            "id                                                           205348\n",
            "minutes                                                           6\n",
            "contributor_id                                               428824\n",
            "submitted                                                2007-01-14\n",
            "n_steps                                                         NaN\n",
            "description       i wrote this because there are an astounding l...\n",
            "n_ingredients                                                   NaN\n",
            "Name: 6779, dtype: object\n",
            "name                                      rich barley mushroom soup\n",
            "id                                                           328708\n",
            "minutes                                                          60\n",
            "contributor_id                                               221776\n",
            "submitted                                                2008-10-03\n",
            "n_steps                                                         NaN\n",
            "description       this is one of the best soups i've ever made a...\n",
            "n_ingredients                                                  10.0\n",
            "Name: 22566, dtype: object\n",
            "name              alligator claws  avocado fritters  with chipot...\n",
            "id                                                           287008\n",
            "minutes                                                          45\n",
            "contributor_id                                               765354\n",
            "submitted                                                2008-02-19\n",
            "n_steps                                                         NaN\n",
            "description       a translucent golden-brown crust allows the gr...\n",
            "n_ingredients                                                   9.0\n",
            "Name: 481, dtype: object\n",
            "name                   my favorite buttercream icing for decorating\n",
            "id                                                           334113\n",
            "minutes                                                          30\n",
            "contributor_id                                               681465\n",
            "submitted                                                2008-10-30\n",
            "n_steps                                                        12.0\n",
            "description       this wonderful icing is used for icing cakes a...\n",
            "n_ingredients                                                   NaN\n",
            "Name: 18408, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-_3FlbqE_up"
      },
      "source": [
        "8\\. Напишите функцию, которая для заданного предложения выводит информацию о частях речи слов, входящих в предложение, в следующем виде:\n",
        "```\n",
        "PRP   VBD   DT      NNS     CC   VBD      NNS        RB   \n",
        " I  omitted the raspberries and added strawberries instead\n",
        "``` \n",
        "Для определения части речи слова можно воспользоваться `nltk.pos_tag`.\n",
        "\n",
        "Проверьте работоспособность функции на названии рецепта с id 241106.\n",
        "\n",
        "Обратите внимание, что часть речи должна находиться ровно посередине над соотвествующим словом, а между самими словами должен быть ровно один пробел.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux0sFz0ME_uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2faf498-6511-416b-91d9-6f2908426092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   JJ      NNS   IN     NNS     VBP   JJ    CC   JJ    NNS  \n",
            "eggplant steaks with chickpeas feta cheese and black olives \n"
          ]
        }
      ],
      "source": [
        "for i in recipes.index:\n",
        "  if recipes.id[i] == 241106:\n",
        "    ind = i\n",
        "a = nltk.pos_tag([i for i in nltk.tokenize.word_tokenize(recipes.name[ind])])\n",
        "s1,s2= '', ''\n",
        "for i,j in dict(a).items():\n",
        "  s1 += f'{j: ^{len(i)+1}}'\n",
        "  s2 += i + ' '\n",
        "print(s1)\n",
        "print(s2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}